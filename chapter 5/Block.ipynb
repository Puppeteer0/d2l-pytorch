{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4202, 0.7647, 0.7307, 0.4656, 0.0819, 0.4563, 0.6220, 0.9963, 0.3975,\n",
      "         0.9949, 0.5859, 0.2683, 0.7215, 0.4810, 0.8902, 0.5751, 0.6566, 0.3803,\n",
      "         0.1560, 0.7273],\n",
      "        [0.4932, 0.1404, 0.3733, 0.7129, 0.7863, 0.3738, 0.8072, 0.5964, 0.6328,\n",
      "         0.8043, 0.2297, 0.1715, 0.9486, 0.8947, 0.7755, 0.5008, 0.2111, 0.5237,\n",
      "         0.5085, 0.7337]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0155, -0.1688, -0.0737, -0.2786,  0.0482,  0.0630,  0.2432, -0.0579,\n",
       "         -0.1456, -0.0576],\n",
       "        [-0.0376, -0.0792, -0.0536, -0.1302, -0.0996,  0.0013,  0.1437, -0.2082,\n",
       "         -0.1118,  0.0252]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "net = nn.Sequential(nn.Linear(20, 256), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "\n",
    "print(X)\n",
    "\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # ⽤模型参数声明层。这⾥，我们声明两个全连接的层\n",
    "    def __init__(self):\n",
    "        # 调⽤MLP的⽗类Module的构造函数来执⾏必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256) # 隐藏层\n",
    "        self.out = nn.Linear(256, 10) # 输出层\n",
    "    # 定义模型的前向传播，即如何根据输⼊X返回所需的模型输出\n",
    "    def forward(self, X):\n",
    "        # 注意，这⾥我们使⽤ReLU的函数版本，其在nn.functional模块中定义。\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0481,  0.0560, -0.0243,  0.0832,  0.0971, -0.2116, -0.2260,  0.3785,\n",
       "          0.0717,  0.0062],\n",
       "        [-0.0282,  0.1119, -0.0372,  0.1661,  0.2180, -0.1804, -0.1990,  0.2745,\n",
       "          0.0773,  0.0381]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4202, 0.7647, 0.7307, 0.4656, 0.0819, 0.4563, 0.6220, 0.9963, 0.3975,\n",
      "         0.9949, 0.5859, 0.2683, 0.7215, 0.4810, 0.8902, 0.5751, 0.6566, 0.3803,\n",
      "         0.1560, 0.7273],\n",
      "        [0.4932, 0.1404, 0.3733, 0.7129, 0.7863, 0.3738, 0.8072, 0.5964, 0.6328,\n",
      "         0.8043, 0.2297, 0.1715, 0.9486, 0.8947, 0.7755, 0.5008, 0.2111, 0.5237,\n",
      "         0.5085, 0.7337]])\n",
      "tensor([[-0.4148,  0.1220,  0.4211,  0.0216, -0.1275, -0.0369, -0.0732,  0.0943,\n",
      "          0.2463, -0.1582,  0.1560,  0.1694, -0.1093, -0.2090,  0.1285,  0.1699,\n",
      "         -0.1338, -0.1841, -0.4152,  0.1556],\n",
      "        [-0.2335,  0.0815,  0.1076,  0.1291, -0.0796,  0.1613,  0.0457,  0.1369,\n",
      "          0.1901, -0.0038, -0.0616,  0.1652, -0.1748, -0.1041, -0.0467,  0.1309,\n",
      "         -0.1962,  0.0547, -0.1783, -0.0956]], grad_fn=<AddmmBackward0>)\n",
      "tensor(2.9618, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.0742, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X)\n",
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使⽤创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复⽤全连接层。这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        print(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum()>1:\n",
    "            X /= 2\n",
    "            print(X.abs().sum())\n",
    "            return X.sum()\n",
    "\n",
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5220,  1.7249, -0.5079,  1.7224,  1.5328, -1.1394, -0.5710,  0.1549,\n",
      "          0.8754,  0.9260, -0.0090, -0.9207,  0.2339,  0.4789,  0.3925,  1.5367,\n",
      "         -1.3741,  0.4403, -0.6714, -0.3673],\n",
      "        [-0.5192,  1.7281, -0.5089,  1.7139,  1.5229, -1.1292, -0.5709,  0.1597,\n",
      "          0.8748,  0.9185, -0.0074, -0.9213,  0.2362,  0.4850,  0.3913,  1.5307,\n",
      "         -1.3705,  0.4404, -0.6751, -0.3661]], grad_fn=<AddmmBackward0>)\n",
      "tensor(16.0860, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.9346, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "    \n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
